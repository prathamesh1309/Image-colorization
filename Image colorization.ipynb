{"cells":[{"cell_type":"markdown","metadata":{},"source":["\n","# Image Colorization using GANs\n","\n","## Installing Dependencies "]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-06-26T12:32:43.182575Z","iopub.status.busy":"2022-06-26T12:32:43.181737Z","iopub.status.idle":"2022-06-26T12:32:45.046486Z","shell.execute_reply":"2022-06-26T12:32:45.045618Z","shell.execute_reply.started":"2022-06-26T12:32:43.182482Z"},"trusted":true},"outputs":[],"source":["import os\n","import glob\n","import time\n","import numpy as np\n","from PIL import Image\n","from pathlib import Path\n","from tqdm.notebook import tqdm\n","import matplotlib.pyplot as plt\n","from skimage.color import rgb2lab, lab2rgb\n","\n","import torch\n","from torch import nn, optim\n","from torchvision import transforms\n","from torchvision.utils import make_grid\n","from torch.utils.data import Dataset, DataLoader\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{},"source":["## Downloading the dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-06-26T12:32:45.049091Z","iopub.status.busy":"2022-06-26T12:32:45.048509Z","iopub.status.idle":"2022-06-26T12:37:42.164314Z","shell.execute_reply":"2022-06-26T12:37:42.163458Z","shell.execute_reply.started":"2022-06-26T12:32:45.049055Z"},"trusted":true},"outputs":[],"source":["from fastai.data.external import untar_data, URLs\n","coco_path = untar_data(URLs.COCO_SAMPLE)\n","coco_path = str(coco_path) + \"/train_sample\"\n","paths = glob.glob(coco_path + \"/*.jpg\")\n","np.random.seed(123)\n","paths_subset = np.random.choice(paths, 10_000, replace=False) \n","rand_idxs = np.random.permutation(10_000)\n","train_idxs = rand_idxs[:8000] \n","val_idxs = rand_idxs[8000:] \n","train_paths = paths_subset[train_idxs]\n","val_paths = paths_subset[val_idxs]\n","print(len(train_paths), len(val_paths))"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-06-26T12:37:42.167278Z","iopub.status.busy":"2022-06-26T12:37:42.166213Z","iopub.status.idle":"2022-06-26T12:37:44.402223Z","shell.execute_reply":"2022-06-26T12:37:44.399946Z","shell.execute_reply.started":"2022-06-26T12:37:42.167240Z"},"trusted":true},"outputs":[],"source":["_, axes = plt.subplots(4, 4, figsize=(10, 10))\n","for ax, img_path in zip(axes.flatten(), train_paths):\n","    ax.imshow(Image.open(img_path))\n","    ax.axis(\"off\")"]},{"cell_type":"markdown","metadata":{},"source":["## Creating Dataloaders"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-06-26T12:37:44.403571Z","iopub.status.busy":"2022-06-26T12:37:44.403176Z","iopub.status.idle":"2022-06-26T12:37:44.419866Z","shell.execute_reply":"2022-06-26T12:37:44.419176Z","shell.execute_reply.started":"2022-06-26T12:37:44.403528Z"},"trusted":true},"outputs":[],"source":["SIZE = 256\n","class ColorizationDataset(Dataset):\n","    def __init__(self, paths, split='train'):\n","        if split == 'train':\n","            self.transforms = transforms.Compose([\n","                transforms.Resize((SIZE, SIZE),  Image.BICUBIC),\n","                transforms.RandomHorizontalFlip(), # A little data augmentation!\n","            ])\n","        elif split == 'val':\n","            self.transforms = transforms.Resize((SIZE, SIZE),  Image.BICUBIC)\n","        \n","        self.split = split\n","        self.size = SIZE\n","        self.paths = paths\n","    \n","    def __getitem__(self, idx):\n","        img = Image.open(self.paths[idx]).convert(\"RGB\")\n","        img = self.transforms(img)\n","        img = np.array(img)\n","        img_lab = rgb2lab(img).astype(\"float32\") # Converting RGB to L*a*b\n","        img_lab = transforms.ToTensor()(img_lab)\n","        L = img_lab[[0], ...] / 50. - 1. # Between -1 and 1\n","        ab = img_lab[[1, 2], ...] / 110. # Between -1 and 1\n","        \n","        return {'L': L, 'ab': ab}\n","    \n","    def __len__(self):\n","        return len(self.paths)\n","\n","def make_dataloaders(batch_size=16, n_workers=4, pin_memory=True, **kwargs): # A handy function to make our dataloaders\n","    dataset = ColorizationDataset(**kwargs)\n","    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=n_workers,\n","                            pin_memory=pin_memory)\n","    return dataloader"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-06-26T12:37:44.427339Z","iopub.status.busy":"2022-06-26T12:37:44.426506Z","iopub.status.idle":"2022-06-26T12:37:53.523707Z","shell.execute_reply":"2022-06-26T12:37:53.521637Z","shell.execute_reply.started":"2022-06-26T12:37:44.427302Z"},"trusted":true},"outputs":[],"source":["train_dl = make_dataloaders(paths=train_paths, split='train')\n","val_dl = make_dataloaders(paths=val_paths, split='val')\n","\n","data = next(iter(train_dl))\n","Ls, abs_ = data['L'], data['ab']\n","print(Ls.shape, abs_.shape)\n","print(len(train_dl), len(val_dl))"]},{"cell_type":"markdown","metadata":{},"source":["## Defining the UNet Generator"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-06-26T12:37:53.526966Z","iopub.status.busy":"2022-06-26T12:37:53.526343Z","iopub.status.idle":"2022-06-26T12:37:53.563080Z","shell.execute_reply":"2022-06-26T12:37:53.562330Z","shell.execute_reply.started":"2022-06-26T12:37:53.526902Z"},"trusted":true},"outputs":[],"source":["class UnetBlock(nn.Module):\n","    def __init__(self, nf, ni, submodule=None, input_c=None, dropout=False,\n","                 innermost=False, outermost=False):\n","        super().__init__()\n","        self.outermost = outermost\n","        if input_c is None: input_c = nf\n","        downconv = nn.Conv2d(input_c, ni, kernel_size=4,\n","                             stride=2, padding=1, bias=False)\n","        downrelu = nn.LeakyReLU(0.2, True)\n","        downnorm = nn.BatchNorm2d(ni)\n","        uprelu = nn.ReLU(True)\n","        upnorm = nn.BatchNorm2d(nf)\n","        \n","        if outermost:\n","            upconv = nn.ConvTranspose2d(ni * 2, nf, kernel_size=4,\n","                                        stride=2, padding=1)\n","            down = [downconv]\n","            up = [uprelu, upconv, nn.Tanh()]\n","            model = down + [submodule] + up\n","        elif innermost:\n","            upconv = nn.ConvTranspose2d(ni, nf, kernel_size=4,\n","                                        stride=2, padding=1, bias=False)\n","            down = [downrelu, downconv]\n","            up = [uprelu, upconv, upnorm]\n","            model = down + up\n","        else:\n","            upconv = nn.ConvTranspose2d(ni * 2, nf, kernel_size=4,\n","                                        stride=2, padding=1, bias=False)\n","            down = [downrelu, downconv, downnorm]\n","            up = [uprelu, upconv, upnorm]\n","            if dropout: up += [nn.Dropout(0.5)]\n","            model = down + [submodule] + up\n","        self.model = nn.Sequential(*model)\n","    \n","    def forward(self, x):\n","        if self.outermost:\n","            return self.model(x)\n","        else:\n","            return torch.cat([x, self.model(x)], 1)\n","\n","class Unet(nn.Module):\n","    def __init__(self, input_c=1, output_c=2, n_down=8, num_filters=64):\n","        super().__init__()\n","        unet_block = UnetBlock(num_filters * 8, num_filters * 8, innermost=True)\n","        for _ in range(n_down - 5):\n","            unet_block = UnetBlock(num_filters * 8, num_filters * 8, submodule=unet_block, dropout=True)\n","        out_filters = num_filters * 8\n","        for _ in range(3):\n","            unet_block = UnetBlock(out_filters // 2, out_filters, submodule=unet_block)\n","            out_filters //= 2\n","        self.model = UnetBlock(output_c, out_filters, input_c=input_c, submodule=unet_block, outermost=True)\n","    \n","    def forward(self, x):\n","        return self.model(x)"]},{"cell_type":"markdown","metadata":{},"source":["## Defining the PatchGAN Discriminator"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-06-26T12:37:53.569246Z","iopub.status.busy":"2022-06-26T12:37:53.566260Z","iopub.status.idle":"2022-06-26T12:37:53.983942Z","shell.execute_reply":"2022-06-26T12:37:53.982906Z","shell.execute_reply.started":"2022-06-26T12:37:53.569208Z"},"trusted":true},"outputs":[],"source":["class PatchDiscriminator(nn.Module):\n","    def __init__(self, input_c, num_filters=64, n_down=3):\n","        super().__init__()\n","        model = [self.get_layers(input_c, num_filters, norm=False)]\n","        model += [self.get_layers(num_filters * 2 ** i, num_filters * 2 ** (i + 1), s=1 if i == (n_down-1) else 2) \n","                          for i in range(n_down)] # the 'if' statement is taking care of not using\n","                                                  # stride of 2 for the last block in this loop\n","        model += [self.get_layers(num_filters * 2 ** n_down, 1, s=1, norm=False, act=False)] # Make sure to not use normalization or\n","                                                                                             # activation for the last layer of the model\n","        self.model = nn.Sequential(*model)                                                   \n","        \n","    def get_layers(self, ni, nf, k=4, s=2, p=1, norm=True, act=True): # when needing to make some repeatitive blocks of layers,\n","        layers = [nn.Conv2d(ni, nf, k, s, p, bias=not norm)]          # it's always helpful to make a separate method for that purpose\n","        if norm: layers += [nn.BatchNorm2d(nf)]\n","        if act: layers += [nn.LeakyReLU(0.2, True)]\n","        return nn.Sequential(*layers)\n","    \n","    def forward(self, x):\n","        return self.model(x)"]},{"cell_type":"markdown","metadata":{},"source":["## Defining the GAN loss function"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-06-26T12:37:53.986130Z","iopub.status.busy":"2022-06-26T12:37:53.985710Z","iopub.status.idle":"2022-06-26T12:37:54.023035Z","shell.execute_reply":"2022-06-26T12:37:54.022250Z","shell.execute_reply.started":"2022-06-26T12:37:53.986095Z"},"trusted":true},"outputs":[],"source":["class GANLoss(nn.Module):\n","    def __init__(self, gan_mode='vanilla', real_label=1.0, fake_label=0.0):\n","        super().__init__()\n","        self.register_buffer('real_label', torch.tensor(real_label))\n","        self.register_buffer('fake_label', torch.tensor(fake_label))\n","        if gan_mode == 'vanilla':\n","            self.loss = nn.BCEWithLogitsLoss()\n","        elif gan_mode == 'lsgan':\n","            self.loss = nn.MSELoss()\n","    \n","    def get_labels(self, preds, target_is_real):\n","        if target_is_real:\n","            labels = self.real_label\n","        else:\n","            labels = self.fake_label\n","        return labels.expand_as(preds)\n","    \n","    def __call__(self, preds, target_is_real):\n","        labels = self.get_labels(preds, target_is_real)\n","        loss = self.loss(preds, labels)\n","        return loss"]},{"cell_type":"markdown","metadata":{},"source":["## Initialising the model"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-06-26T12:37:54.030299Z","iopub.status.busy":"2022-06-26T12:37:54.027092Z","iopub.status.idle":"2022-06-26T12:37:54.069281Z","shell.execute_reply":"2022-06-26T12:37:54.068371Z","shell.execute_reply.started":"2022-06-26T12:37:54.030258Z"},"trusted":true},"outputs":[],"source":["def init_weights(net, init='norm', gain=0.02):\n","    \n","    def init_func(m):\n","        classname = m.__class__.__name__\n","        if hasattr(m, 'weight') and 'Conv' in classname:\n","            if init == 'norm':\n","                nn.init.normal_(m.weight.data, mean=0.0, std=gain)\n","            elif init == 'xavier':\n","                nn.init.xavier_normal_(m.weight.data, gain=gain)\n","            elif init == 'kaiming':\n","                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n","            \n","            if hasattr(m, 'bias') and m.bias is not None:\n","                nn.init.constant_(m.bias.data, 0.0)\n","        elif 'BatchNorm2d' in classname:\n","            nn.init.normal_(m.weight.data, 1., gain)\n","            nn.init.constant_(m.bias.data, 0.)\n","            \n","    net.apply(init_func)\n","    print(f\"model initialized with {init} initialization\")\n","    return net\n","\n","def init_model(model, device):\n","    model = model.to(device)\n","    model = init_weights(model)\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["## Implementing the model"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-06-26T12:37:54.073245Z","iopub.status.busy":"2022-06-26T12:37:54.072861Z","iopub.status.idle":"2022-06-26T12:37:54.110442Z","shell.execute_reply":"2022-06-26T12:37:54.108263Z","shell.execute_reply.started":"2022-06-26T12:37:54.073215Z"},"trusted":true},"outputs":[],"source":["class MainModel(nn.Module):\n","    def __init__(self, net_G=None, lr_G=2e-4, lr_D=2e-4, \n","                 beta1=0.5, beta2=0.999, lambda_L1=100.):\n","        super().__init__()\n","        \n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.lambda_L1 = lambda_L1\n","        \n","        if net_G is None:\n","            self.net_G = init_model(Unet(input_c=1, output_c=2, n_down=8, num_filters=64), self.device)\n","        else:\n","            self.net_G = net_G.to(self.device)\n","        self.net_D = init_model(PatchDiscriminator(input_c=3, n_down=3, num_filters=64), self.device)\n","        self.GANcriterion = GANLoss(gan_mode='vanilla').to(self.device)\n","        self.L1criterion = nn.L1Loss()\n","        self.opt_G = optim.Adam(self.net_G.parameters(), lr=lr_G, betas=(beta1, beta2))\n","        self.opt_D = optim.Adam(self.net_D.parameters(), lr=lr_D, betas=(beta1, beta2))\n","    \n","    def set_requires_grad(self, model, requires_grad=True):\n","        for p in model.parameters():\n","            p.requires_grad = requires_grad\n","        \n","    def setup_input(self, data):\n","        self.L = data['L'].to(self.device)\n","        self.ab = data['ab'].to(self.device)\n","        \n","    def forward(self):\n","        self.fake_color = self.net_G(self.L)\n","    \n","    def backward_D(self):\n","        fake_image = torch.cat([self.L, self.fake_color], dim=1)\n","        fake_preds = self.net_D(fake_image.detach())\n","        self.loss_D_fake = self.GANcriterion(fake_preds, False)\n","        real_image = torch.cat([self.L, self.ab], dim=1)\n","        real_preds = self.net_D(real_image)\n","        self.loss_D_real = self.GANcriterion(real_preds, True)\n","        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n","        self.loss_D.backward()\n","    \n","    def backward_G(self):\n","        fake_image = torch.cat([self.L, self.fake_color], dim=1)\n","        fake_preds = self.net_D(fake_image)\n","        self.loss_G_GAN = self.GANcriterion(fake_preds, True)\n","        self.loss_G_L1 = self.L1criterion(self.fake_color, self.ab) * self.lambda_L1\n","        self.loss_G = self.loss_G_GAN + self.loss_G_L1\n","        self.loss_G.backward()\n","    \n","    def optimize(self):\n","        self.forward()\n","        self.net_D.train()\n","        self.set_requires_grad(self.net_D, True)\n","        self.opt_D.zero_grad()\n","        self.backward_D()\n","        self.opt_D.step()\n","        \n","        self.net_G.train()\n","        self.set_requires_grad(self.net_D, False)\n","        self.opt_G.zero_grad()\n","        self.backward_G()\n","        self.opt_G.step()"]},{"cell_type":"markdown","metadata":{},"source":["## Helper Functions"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-06-26T12:37:54.113892Z","iopub.status.busy":"2022-06-26T12:37:54.113590Z","iopub.status.idle":"2022-06-26T12:37:54.154666Z","shell.execute_reply":"2022-06-26T12:37:54.153920Z","shell.execute_reply.started":"2022-06-26T12:37:54.113863Z"},"trusted":true},"outputs":[],"source":["class AverageMeter:\n","    def __init__(self):\n","        self.reset()\n","        \n","    def reset(self):\n","        self.count, self.avg, self.sum = [0.] * 3\n","    \n","    def update(self, val, count=1):\n","        self.count += count\n","        self.sum += count * val\n","        self.avg = self.sum / self.count\n","\n","def create_loss_meters():\n","    loss_D_fake = AverageMeter()\n","    loss_D_real = AverageMeter()\n","    loss_D = AverageMeter()\n","    loss_G_GAN = AverageMeter()\n","    loss_G_L1 = AverageMeter()\n","    loss_G = AverageMeter()\n","    \n","    return {'loss_D_fake': loss_D_fake,\n","            'loss_D_real': loss_D_real,\n","            'loss_D': loss_D,\n","            'loss_G_GAN': loss_G_GAN,\n","            'loss_G_L1': loss_G_L1,\n","            'loss_G': loss_G}\n","\n","\n","\n","def update_losses(model, loss_meter_dict, count):\n","    for loss_name, loss_meter in loss_meter_dict.items():\n","        loss = getattr(model, loss_name)\n","        loss_meter.update(loss.item(), count=count)\n","\n","def lab_to_rgb(L, ab):\n","    \"\"\"\n","    Takes a batch of images\n","    \"\"\"\n","    \n","    L = (L + 1.) * 50.\n","    ab = ab * 110.\n","    Lab = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().numpy()\n","    rgb_imgs = []\n","    for img in Lab:\n","        img_rgb = lab2rgb(img)\n","        rgb_imgs.append(img_rgb)\n","    return np.stack(rgb_imgs, axis=0)\n","    \n","def visualize(model, data, save=True):\n","    model.net_G.eval()\n","    with torch.no_grad():\n","        model.setup_input(data)\n","        model.forward()\n","    model.net_G.train()\n","    fake_color = model.fake_color.detach()\n","    real_color = model.ab\n","    L = model.L\n","    fake_imgs = lab_to_rgb(L, fake_color)\n","    real_imgs = lab_to_rgb(L, real_color)\n","    fig = plt.figure(figsize=(15, 8))\n","    for i in range(5):\n","        ax = plt.subplot(3, 5, i + 1)\n","        ax.imshow(L[i][0].cpu(), cmap='gray')\n","        ax.axis(\"off\")\n","        ax = plt.subplot(3, 5, i + 1 + 5)\n","        ax.imshow(fake_imgs[i])\n","        ax.axis(\"off\")\n","        ax = plt.subplot(3, 5, i + 1 + 10)\n","        ax.imshow(real_imgs[i])\n","        ax.axis(\"off\")\n","    plt.show()\n","    if save:\n","        fig.savefig(f\"colorization_{time.time()}.png\")\n","        \n","def log_results(loss_meter_dict):\n","    for loss_name, loss_meter in loss_meter_dict.items():\n","        print(f\"{loss_name}: {loss_meter.avg:.5f}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Model training"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-06-26T12:37:54.159717Z","iopub.status.busy":"2022-06-26T12:37:54.158023Z","iopub.status.idle":"2022-06-26T15:25:14.265358Z","shell.execute_reply":"2022-06-26T15:25:14.264428Z","shell.execute_reply.started":"2022-06-26T12:37:54.159677Z"},"trusted":true},"outputs":[],"source":["losses_g = []\n","losses_d = []\n","def train_model(model, train_dl, epochs, display_every=250):\n","    data = next(iter(val_dl)) \n","    for e in range(epochs):\n","        loss_meter_dict = create_loss_meters() \n","        i = 0                                  \n","        for data in tqdm(train_dl):\n","            model.setup_input(data) \n","            model.optimize()\n","            update_losses(model, loss_meter_dict, count=data['L'].size(0)) \n","            \n","            i += 1\n","            if i % display_every == 0:\n","                losses_g.append(getattr(model, 'loss_G').cpu().detach().numpy())            \n","                losses_d.append(getattr(model, 'loss_D').cpu().detach().numpy())\n","                print(f\"\\nEpoch {e+1}/{epochs}\")\n","                print(f\"Iteration {i}/{len(train_dl)}\")\n","                log_results(loss_meter_dict) \n","                visualize(model, data, save=False) \n","\n","model = MainModel()\n","train_model(model, train_dl, 50)"]},{"cell_type":"markdown","metadata":{},"source":["## Plotiing the losses"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-06-26T15:25:14.268126Z","iopub.status.busy":"2022-06-26T15:25:14.267786Z","iopub.status.idle":"2022-06-26T15:25:14.468514Z","shell.execute_reply":"2022-06-26T15:25:14.467668Z","shell.execute_reply.started":"2022-06-26T15:25:14.268096Z"},"trusted":true},"outputs":[],"source":["plt.plot(losses_d, '-')\n","plt.plot(losses_g, '-')\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.legend(['Discriminator', 'Generator'])\n","plt.title('Losses');\n"]},{"cell_type":"markdown","metadata":{},"source":["## Building the Unet with ResNet backbone"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-06-26T15:25:14.471668Z","iopub.status.busy":"2022-06-26T15:25:14.471385Z","iopub.status.idle":"2022-06-26T15:25:15.205930Z","shell.execute_reply":"2022-06-26T15:25:15.205016Z","shell.execute_reply.started":"2022-06-26T15:25:14.471644Z"},"trusted":true},"outputs":[],"source":["from fastai.vision.learner import create_body\n","from torchvision.models.resnet import resnet18\n","from fastai.vision.models.unet import DynamicUnet\n","\n","def build_res_unet(n_input=1, n_output=2, size=256):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    body = create_body(resnet18, pretrained=True, n_in=n_input, cut=-2)\n","    net_G = DynamicUnet(body, n_output, (size, size)).to(device)\n","    return net_G"]},{"cell_type":"markdown","metadata":{},"source":["## Pretraining the Generator"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-06-26T15:25:15.207720Z","iopub.status.busy":"2022-06-26T15:25:15.207351Z","iopub.status.idle":"2022-06-26T16:33:04.152839Z","shell.execute_reply":"2022-06-26T16:33:04.151893Z","shell.execute_reply.started":"2022-06-26T15:25:15.207682Z"},"trusted":true},"outputs":[],"source":["def pretrain_generator(net_G, train_dl, opt, criterion, epochs):\n","    for e in range(epochs):\n","        loss_meter = AverageMeter()\n","        for data in tqdm(train_dl):\n","            L, ab = data['L'].to(device), data['ab'].to(device)\n","            preds = net_G(L)\n","            loss = criterion(preds, ab)\n","            opt.zero_grad()\n","            loss.backward()\n","            opt.step()\n","            \n","            loss_meter.update(loss.item(), L.size(0))\n","            \n","        print(f\"Epoch {e + 1}/{epochs}\")\n","        print(f\"L1 Loss: {loss_meter.avg:.5f}\")\n","\n","net_G = build_res_unet(n_input=1, n_output=2, size=256)\n","opt = optim.Adam(net_G.parameters(), lr=1e-4)\n","criterion = nn.L1Loss()        \n","pretrain_generator(net_G, train_dl, opt, criterion, 20)\n","torch.save(net_G.state_dict(), \"res18-unet.pt\")"]},{"cell_type":"markdown","metadata":{},"source":["## Training the new model"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-06-26T16:33:04.155075Z","iopub.status.busy":"2022-06-26T16:33:04.154427Z","iopub.status.idle":"2022-06-26T17:54:37.045618Z","shell.execute_reply":"2022-06-26T17:54:37.044690Z","shell.execute_reply.started":"2022-06-26T16:33:04.155033Z"},"trusted":true},"outputs":[],"source":["net_G = build_res_unet(n_input=1, n_output=2, size=256)\n","net_G.load_state_dict(torch.load(\"res18-unet.pt\", map_location=device))\n","model = MainModel(net_G=net_G)\n","train_model(model, train_dl, 20)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
